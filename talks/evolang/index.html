<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
 
<title>Words as unmotivated cues</title>
 
<meta name="description" content="Words as unmotivated cues">    
 
  <meta name="author" content="Pierce Edmiston and Gary Lupyan" />
 
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
 
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
 
<link rel="stylesheet" href="css/reveal.min.css">
  <link rel="stylesheet" href="css/theme/solarized.css" id="theme">
 
 
<!-- For syntax highlighting -->
  <link rel="stylesheet" href="lib/css/zenburn.css">
 
 
<!-- If the query includes 'print-pdf', use the PDF print sheet -->
<script>
  document.write( '<link rel="stylesheet" href="css/print/' +
    ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + 
    '.css" type="text/css" media="print">' );
</script>
 
<!--[if lt IE 9]>
<script src="lib/js/html5shiv.js"></script>
<![endif]-->
</head>
 
<body>
 
<div class="reveal">
 
<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">
 
<section>
<h1>Words as unmotivated cues</h1>
<h3>Pierce Edmiston and Gary Lupyan</h3>
<p>
<h4>EvoLang X, April 2014</h4>
</p>
</section>  
 
 
<section id="intro" class="level1">
<h1>About</h1>
<figure>
<img src="./images/uwlogo.png" />
</figure>
<ul>
<li>Department of Psychology</li>
<li><a href="www.sapir.psych.wisc.edu">Gary Lupyan's Lab</a></li>
</ul>
<aside class="notes">
    
I'm Pierce Edmiston, coming here from Madison, Wisconsin and I'll be presenting some work I've been doing with Gary Lupyan.
</aside>

</section>
<section id="facet" class="level1">
<h1>Words as unmotivated cues</h1>
<figure>
<img src="./images/facets.png" alt="A facet of language" /><figcaption>A facet of language</figcaption>
</figure>
<aside class="notes">
    
My talk today is going to be about a particular facet of language. I'm going to describe this facet by comparing words to other types of nonverbal information. I'll summarize this comparison by calling words &quot;unmotivated cues&quot;, and I'll get to a more precise definition of what makes a cue motivated or unmotivated in a minute, but for now I just want you to know that the purpose of this research is to explore a particular way in which words activate what we know differently than just about every other type of information in our everyday environment. Now the relevance of this distinction for this conference is in what this difference allows us to do with language, and I would like to argue that this little facet of language---that words are unmotivated cues---is crucial to understanding how language diverged from other types of nonverbal communication.
</aside>

</section>
<section id="whats-an-unmotivated-cue" class="level1">
<h1>What's an unmotivated cue?</h1>
<section id="section" class="level2">
<h2></h2>
<aside class="notes">
    
So what is an unmotivated cue. To introduce the concept of an unmotivated cue, I'd like to use a very simple example.
</aside>

</section>
<section id="section-1" class="level2">
<h2></h2>
<figure>
<img src="./images/dog-park.png" alt="&quot;dog&quot; versus &lt;bark&gt;" /><figcaption><code>&quot;dog&quot; versus &lt;bark&gt;</code></figcaption>
</figure>
<aside class="notes">
    
Imagine coming up to a dog park and hearing two things: the word dog from the person right next to you and the sound of one of these dog's barking. Independently, we know that these two auditory cues are associated with the same category of animals in the world. Dogs are by definition called &quot;dogs&quot; and the natural sound of a dog bark is highly likely to have come from an actual dog. However, there is a very important difference between the two. The difference is that the bark identifies an individual dog---it both selects one of those eight dogs but it also puts a blip on our mental radar, so to speak, about the existence of that dog in our proximate environment.
</aside>

</section>
<section id="section-2" class="level2">
<h2></h2>
<table>
<col style="width: 34%" /><col style="width: 30%" /><thead>
<tr class="header">
<th style="text-align: left;"><code>&lt;bark&gt;</code></th>
<th style="text-align: left;">&quot;dog&quot;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p>1/8 + <br> size of dog + <br> location of dog + <br> &quot;state&quot; of dog</p></td>
<td style="text-align: left;"><p>8/8 + <br> dog at home + <br> cartoon dogs + <br> imaginary dogs</p></td>
</tr>
</tbody>
</table>
<aside class="notes">
    
The word &quot;dog&quot; does not do this. The word &quot;dog&quot; identifies each dog and in sum identifies all of the dogs, plus it brings to mind our own dog back at home, cartoon dogs, even imaginary dogs. Now the experiments I'm going to tell you about directly compare these two types of cues along these dimensions, but before I get to the data, I'd like to take a moment to pause on where these differences in meaning come from.
</aside>

</section>
</section>
<section id="differences-are-learned" class="level1">
<h1>Differences are learned</h1>
<ul>
<li>words
<ul>
<li>also known as &quot;learning a language&quot;!!</li>
<li>words are cues to meaning</li>
</ul></li>
<li>sounds
<ul>
<li>perception is interpreting the source</li>
<li><em>the metamodal brain</em></li>
</ul></li>
</ul>
<aside class="notes">
    
Now it's important to note that we don't have to posit any universals about auditory waveforms to explain the difference between words and sounds. Verbal words and nonverbal sounds are learned very differently. Learning to associate a particular word form with, for example, all of the dogs in front of us and dogs back at home and dogs on TV is a fundamental part of learning a language. Environmental sounds, on the other hand, are causal auditory events. We learn what these sounds mean in the context of the relationship that it has with its source.
</aside>

</section>
<section id="shape-from-sound" class="level1">
<h1>Shape from sound</h1>
<ul>
<li>size of dog from bark</li>
<li>shape of &quot;gongs&quot;</li>
<li>length of rods</li>
<li>quarters, dimes, nickels, pennies</li>
</ul>
<figure>
<img src="./images/kunkler-peck.png" />
</figure>
<aside class="notes">
    
What I mean by relationship with its source is really crucial to why Gary and I chose to use the terms motivated and unmotivated rather than icons and symbols, though I will be the first to admit that the distinction is in emphasis not in kind. So what I mean by the relationship with its source is that a dog bark is an informationally dense cue. The acoustic form of a dog's bark is a direct result of the dog's body: it's vocal tract length and mass, and it's existence as a discrete object in space. What this means is that the difference between two dog barks, by the laws of physics, corresponds to some difference between those two dogs. Now the important thing is that if we grow up listening to dog barks and seeing dogs, we pick up on this stuff, and it's not just about guessing the size of a dog from its bark. For example, people are pretty good at guessing the shape of a metallic gong or the identify of quarters, dimes, and nickels from the sound they make when dropped. The point is not to say that our perception of these events is veridical in any way, the point is that we learn the relationships between shape and sound in such a way that within-category variation is meaningful.
</aside>

</section>
<section id="hypothesis" class="level1">
<h1>Hypothesis</h1>
<section id="section-3" class="level2">
<h2></h2>
<figure>
<img src="./images/unmotivated.png" />
</figure>
<aside class="notes">
    
To formalize this notion of motivated and unmotivated cues, I'd like you to compare the relationships between three tokens of the spoken word dog and three tokens of dog barks with three dogs in our environment. As we just saw, a differences between the dog barks covaries with some difference in the actual dogs that produced the sounds. We say these are motivated cues, and it includes not just dog barks but any highly diagnostic environmental sounds, such as bird chirps, chainsaw revs, a baby crying, etc. In contrast, words are unmotivated cues: the difference between two tokens of the word &quot;dog&quot; does not need to correspond to a difference between the two dogs. Of course, spoken words are still physically produced, and differences between speech acts are &quot;motivated cues&quot; for an individual person, and we have no problem telling things like gender or approximate age from the sound of someone's voice. But this difference is really about how these two types of cues access what we know about dogs.
</aside>

</section>
<section id="sounds-cue-category-members-words-cue-categories" class="level2">
<h2>Sounds cue category members; words cue categories</h2>
<ul>
<li>compare unmotivated to motivated cues</li>
<li>what do words do that sounds can't?</li>
</ul>
<aside class="notes">
   
At this point the natural question is: so what? How or why does the fact that words are unmotivated matter? The experiments I'm going to tell you about are the tip of the iceberg in getting to the bottom of that question, and the basic approach I'm going to take is to compare words to equally diagnostic and familiar sounds in a variety of tasks. So these are environmental sounds from both animate and inanimate sources that have been normed to have a high degree of recognizability for a particular category of objects. And the question I want you to keep in might throughout is this: What do words do that sounds can't?
</aside>

</section>
</section>
<section id="dog-versus-bark" class="level1">
<h1><code>&quot;dog&quot; versus &lt;bark&gt;</code></h1>
<section id="section-4" class="level2">
<h2></h2>
<figure>
<img src="./images/pic-verification.png" alt="Sound-Picture Verification Task" /><figcaption>Sound-Picture Verification Task</figcaption>
</figure>
<aside class="notes">
    
The first experiments I'm going to tell you about use a simple sound-picture verification task. Participants sit at a computer, they hear an auditory cue through some headphones like a bird chirp or the word baby and then they see a picture. They simply have to respond yes or no does the sound they heard match the picture they saw. The behavior we're interested in is the speed at which someone can correctly identify a picture as a member of the cued category and our logic is this: the easier it is for a cue to activate the relevant visual features of members of a particular category, the faster those pictures can be identified.
</aside>

</section>
<section id="section-5" class="level2">
<h2></h2>
<figure>
<img src="./images/label-advantage.png" alt="from Lupyan &amp; Thompson-Schill (2012)" /><figcaption>from Lupyan &amp; Thompson-Schill (2012)</figcaption>
</figure>
<aside class="notes">
    
Now, when you have people do this task there is a stark label advantage. People are always faster to respond after hearing a label compared to a sound. Importantly it's not just that sounds take longer to process. This label advantage extends even when the participants have a second and half to process the cue.
</aside>

</section>
</section>
<section id="too-precise" class="level1">
<h1>Too precise</h1>
<section id="section-6" class="level2">
<h2></h2>
<aside class="notes">
    
To understand this label advantage we can turn to the difference between words and sounds in motivation. On this view, what makes sounds less effective than labels is that they are too precise. All you need to know in order to do this task is the category of images you are looking for. Maybe it's the case that these informationally dense sounds are telling you more information than you need to know and that is slowing you down.
</aside>

</section>
<section id="section-7" class="level2">
<h2></h2>
<figure>
<img src="./images/sound-congruence-1.png" />
</figure>
<aside class="notes">
    
We can test this by looking at how well sounds and labels cue individual members within the same category. Labels, as we know, are most effective for category typical items, but sounds, as motivated cues, are likely to be the most effective cues for the most likely source of the sound. So what we did was gathered a bunch of images for each category and had some that were better matches to the sound and some that were worse matches to the sound.
</aside>

</section>
<section id="section-8" class="level2">
<h2></h2>
<figure>
<img src="./images/sound-congruence-2.png" />
</figure>
</section>
<section id="section-9" class="level2">
<h2></h2>
<figure>
<img src="./images/exp1-stimuli.png" />
</figure>
<aside class="notes">
    
In the first experiment I'm going to tell you about we compared sounds that were perfectly matched with their source to sounds that were not but still in the same category of objects. A nice example is the category of guitars: we have acoustic guitars and electric guitars---they sound different but are easily recongizable as guitars.
</aside>

</section>
<section id="section-10" class="level2">
<h2></h2>
<figure>
<img src="./images/subordinate-1.png" />
</figure>
<aside class="notes">
    
So we had participants do another sound-picture verification task, and just to reiterate, all the participant has to do is identify either of these guitar sounds as cues for the whole category of guitars, and looking at accuracies people have no problem doing this. However, even though they are tasked with treating these two cues the same, we still see faster responses when the sound matches it source than when it doesn't---people have a hard time extracting category level information from these natural sound cues.
</aside>
    
</section>
<section id="section-11" class="level2">
<h2></h2>
<figure>
<img src="./images/subordinate-2.png" />
</figure>
<aside class="notes">
    
Now what's surprising is that even these congruent sounds still don't make up the difference with labels, even in an experiment where all they hear is congruent sounds.
</aside>

</section>
</section>
<section id="are-sounds-temporally-precise" class="level1">
<h1>Are sounds temporally precise?</h1>
<section id="section-12" class="level2">
<h2></h2>
<aside class="notes">
   
So what else could explain the difference? Well, we wondered if the fact that when we heard a dog bark we usually saw a dog nearby mattered; if the sounds were temporally precise.
</aside>

</section>
<section id="section-13" class="level2">
<h2></h2>
<p>Delayed</p>
<figure>
<img src="./images/pic-verification.png" />
</figure>
<p>Simultaneous</p>
<figure>
<img src="./images/pic-verification-delay.png" />
</figure>
</section>
<section id="section-14" class="level2">
<h2></h2>
<figure>
<img src="./images/timing-1.png" />
</figure>
<aside class="notes">
    
Just to orient you, in this experiment we piloted the images to get a more continuous measure of sound congruence where the images on the right side of this graph are better matches to the sound. And what we're interested in is the relationship between the delayed trials on the left to the simultaneous trials on the right.
</aside>

</section>
<section id="section-15" class="level2">
<h2></h2>
<figure>
<img src="./images/timing-2.png" />
</figure>
<aside class="notes">
    
For the delayed trials labels are still faster than sounds at all levels of sound congruence.
</aside>

</section>
<section id="section-16" class="level2">
<h2></h2>
<figure>
<img src="./images/timing-3.png" />
</figure>
<aside class="notes">
    
However, for the simultaneous trials, we see that sound congruence is a strong predictor response times for sounds but not for labels. In fact, it's only when you have the most sound matched images that the label advantage comes the closest to disappearing.
</aside>

</section>
</section>
<section id="members-versus-categories" class="level1">
<h1>Members versus categories</h1>
<section id="section-17" class="level2">
<h2></h2>
<aside class="notes">
    
Now the last experiment I'm going to tell you about touches on the example I started the talk with: walking to a dog park and hearing a bark. The question we asked with this study is: do people still process the bark as a cue to an individual member even when we put that situation in the lab.
</aside>

</section>
<section id="section-18" class="level2">
<h2></h2>
<figure>
<img src="./images/tys-task-1.png" />
</figure>
<aside class="notes">
    
So to do that we present participants with four pictures and tell them to click on the one that gets highlighted in green. Now while they are doing this we play them auditory cues---either words or sounds---and watch where they move their eyes. Note that in all trials, the pictures were of the same category and there was no relationship between the type of picture and the likelihood it would be the target.
</aside>

</section>
<section id="section-19" class="level2">
<h2></h2>
<figure>
<img src="./images/fixation-average.jpg" />
</figure>
<aside class="notes">
    
However, people still fixate on the most likely source of the sound cue.
</aside>

</section>
<section id="section-20" class="level2">
<h2></h2>
<figure>
<img src="./images/tys-task-2.png" />
</figure>
</section>
<section id="section-21" class="level2">
<h2></h2>
<figure>
<img src="./images/fixation-timecourse.jpg" />
</figure>
<aside class="notes">
    
Now what is interesting is if you take it one step further. So we can bin the four images on each trial in order of increasing sound congruence, and when we look over the course of the trial we see that participants aren't looking at the four images in a graded fashion, they are fixating the most likely source of the image before the offset of the cue and then searching more evenly after that.
</aside>

</section>
</section>
<section id="take-home-messages" class="level1">
<h1>Take home messages</h1>
<pre><code>+-------------------+----------------+
|      &lt;bark&gt;       |      &quot;dog&quot;     |
+===================+================+
| 1/8 +             | 8/8 +          |
| size of dog +     | dog at home +  |
| location of dog + | cartoon dogs + |
| &quot;state&quot; of dog    | imaginary dogs | 
+-------------------+----------------+

My table in Markdown.</code></pre>
<ol type="1">
<li>verbal and nonverbal auditory cues are learned to mean different things due to different regularities our typical experiences</li>
<li>unmotivated are easier to associate with a psychological category of objects
<ul>
<li>motivated cues are too precise</li>
</ul></li>
</ol>
<aside class="notes">
    
</aside>

</section>
<section id="words-as-unmotivated-cues" class="level1">
<h1>Words as unmotivated cues</h1>
<section id="section-22" class="level2">
<h2></h2>
<figure>
<img src="./images/facets.png" />
</figure>
<blockquote>
<p>&quot;...key to early developments in crystallography, since they reflect the underlying symmetry of the crystal structure.&quot;</p>
</blockquote>
</section>
</section>
<section id="snarls-and-snags" class="level1">
<h1>Snarls and snags</h1>
<ul>
<li>content-laden theories of semantics</li>
<li><code>&lt;bark&gt;</code> is misleading
<ul>
<li>we hear barks without dogs all the time</li>
<li><code>&lt;bark&gt;</code> is not a motivated cue for a static 2D image</li>
</ul></li>
</ul>
</section>
</div>
 
<script src="lib/js/head.min.js"></script>
<script src="js/reveal.min.js"></script>
 
<script>
  // Full list of configuration options available here:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: false,
 
  // available themes are in /css/theme
      theme: Reveal.getQueryHash().theme || 'solarized', 
   
  // default/cube/page/concave/zoom/linear/fade/none
      transition: Reveal.getQueryHash().transition || 'linear',
   
  // Optional libraries used to extend on reveal.js
  dependencies: [
    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
    { src: 'plugin/markdown/showdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
    // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
  });
 
</script>
 
</body>
</html>